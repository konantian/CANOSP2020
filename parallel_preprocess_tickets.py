"""
Usage: python parallel_preprocess_tickets.py --input_file data/tickets.csv

The output file is located at data/tickets_preprocessed.csv

The input file is the raw ticket data generated by the ticket_to_csv.py.

In this script, we would preprocess the raw ticket parallely by forking multiple copy of the program
to speed up the processing time.
"""

import os
import json
import csv
import argparse
import multiprocessing
import subprocess
import pathlib

import pandas as pd
import numpy as np
from canosp2020.preprocessing import Preprocess


SCRIPT_NAME = "parallel_preprocess_tickets.py"


def preprocess_df(df):
    preprocessor = Preprocess(df)
    df = preprocessor.preprocess_tickets()
    df = preprocessor._df
    df = df.drop(columns=['ticket_title', 'ticket_content', 'ticket_lang'])
    return df


def worker(input_file):
    return subprocess.call(["python", SCRIPT_NAME, "--input_file", input_file, "--output_file", input_file])


def parallel_preprocess(input_files, num_cores):
    # # Run preprocessing of csv in parallel using subprocess
    pool = multiprocessing.Pool(num_cores)
    pool.map(worker, input_files)
    pool.close()
    pool.join()


def preprocess_file(input_path, output_path):
    input_df = pd.read_csv(input_path)
    output_df = preprocess_df(input_df)
    output_df.to_csv(output_path, index=False)


def split(input_path):
    num_cores = multiprocessing.cpu_count()

    input_filename, input_file_ext = os.path.splitext(os.path.basename(input_path))
    input_dir, _ = os.path.split(input_path)
    output_filename = f"{input_filename}_preprocessed{input_file_ext}"
    output_path = os.path.join(input_dir, output_filename)

    # Split files into N chunck
    input_df = pd.read_csv(input_path)
    df_split = np.array_split(input_df, num_cores)

    if not os.path.exists(".tmp"):
        os.mkdir(".tmp")

    filenames = [f".tmp/{input_filename}.{i}{input_file_ext}" for i in range(len(df_split))]
    for name, df in zip(filenames, df_split):
        df.to_csv(name, index=False)

    # Since spacy and numpy are not optimized for multiprocessing
    # We will use subprocess to fork multiple copy of program
    # to preprocess each chunk of input files
    parallel_preprocess(filenames, num_cores)

    # Join files back together
    all_df = [pd.read_csv(f) for f in filenames]
    output_df = pd.concat(all_df)
    output_df.to_csv(output_path, index=False)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Preprocess text file")
    parser.add_argument("--input_file", help="the relative path to the input CSV file", required=True)
    parser.add_argument(
        "--output_file", help="the relative path to the output CSV file (will be overwritten if exists)"
    )

    args = parser.parse_args()
    path = pathlib.Path()
    input_path = None
    output_path = None

    if args.input_file:
        input_path = path / args.input_file
    if args.output_file:
        output_path = path / args.output_file

    if input_path and not output_path:
        split(input_path)
    elif input_path and output_path:
        preprocess_file(input_path, output_path)
