{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ingest Data\n",
    "Describes the Data ingestions strategy\n",
    "\n",
    "- What subset of the available corpus is used?\n",
    "- What are the criteria defined for measuring performance?\n",
    "- Is filtering on date, version, tags employed?\n",
    "- Size of the data being used in this demo\n",
    "\n",
    "The tickets that were used to train model: \"sample_data.json\".\n",
    "No filtering was employed. Only the ticket text (tittle and body) were used.\n",
    "The number of tickets used to train the model: 31794 tickets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import re\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get data for training\"\"\"\n",
    "\n",
    "with open(\"data/sample_data1.json\") as f:\n",
    "  data = json.load(f)\n",
    "\n",
    "tickets = {}\n",
    "tickets = data['tickets']\n",
    "\n",
    "with open('data/ticket_data.json', 'w') as json_file:\n",
    "  json.dump(tickets, json_file)\n",
    "\n",
    "\n",
    "\"\"\"Get first 500 tickets from all of the data for the keyword extraction\"\"\"\n",
    "sample_tickets = []\n",
    "sample_tickets =  tickets[0:500]\n",
    "\n",
    "with open('data/sample_ticket_data.json', 'w') as json_file:\n",
    "  json.dump(sample_tickets, json_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing and Feature Engineering steps\n",
    "\n",
    "Inlcude the series of data cleaning steps that are included in your workflow\n",
    "\n",
    "- stopword removal strategy\n",
    "- tokenization/stemming strategy\n",
    "- normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_process(text):\n",
    "    \n",
    "    # lowercase\n",
    "    text=text.lower()\n",
    "    \n",
    "    #remove tags\n",
    "    text=re.sub(\"</?.*?>\",\" <> \",text)\n",
    "    \n",
    "    # remove special characters and digits\n",
    "    text=re.sub(\"(\\\\d|\\\\W)+\",\" \",text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Schema:\n",
      "\n",
      " ticket_id             int64\n",
      "title                object\n",
      "content              object\n",
      "timestamp    datetime64[ns]\n",
      "tags                 object\n",
      "dtype: object\n",
      "Number of questions,columns= (31794, 5)\n"
     ]
    }
   ],
   "source": [
    "# read json into a dataframe\n",
    "df_idf = pd.read_json(\"data/ticket_data.json\")\n",
    "\n",
    "# print schema\n",
    "print(\"Schema:\\n\\n\",df_idf.dtypes)\n",
    "print(\"Number of questions,columns=\",df_idf.shape)\n",
    "\n",
    "df_idf['text'] = df_idf['title'] + df_idf['content']\n",
    "df_idf['text'] = df_idf['text'].apply(lambda x:pre_process(x))\n",
    "\n",
    "#show the first 'text'\n",
    "#df_idf['text'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get the text column \n",
    "docs=df_idf['text'].tolist()\n",
    "\n",
    "#create a vocabulary of words, \n",
    "#ignore words that appear in 85% of documents, \n",
    "#eliminate stop words\n",
    "cv=CountVectorizer(max_df=0.90,stop_words=stopwords.words('english'), min_df=1)\n",
    "word_count_vector=cv.fit_transform(docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read test docs into a dataframe and concatenate title and body\n",
    "df_test=pd.read_json(\"data/sample_ticket_data.json\")\n",
    "df_test['text'] = df_test['title'] + df_test['content']\n",
    "df_test['text'] =df_test['text'].apply(lambda x:pre_process(x))\n",
    "\n",
    "# get test docs into a list\n",
    "docs_test=df_test['text'].tolist()\n",
    "docs_title=df_test['title'].tolist()\n",
    "docs_body=df_test['content'].tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tag lexicon definition description\n",
    "\n",
    "Define the set of tags that are defined for the task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.57530485,  9.57530485, 10.26845204, ..., 10.67391714,\n",
       "       10.67391714, 10.67391714])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True)\n",
    "tfidf_transformer.fit(word_count_vector)\n",
    "\n",
    "tfidf_transformer.idf_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_coo(coo_matrix):\n",
    "    tuples = zip(coo_matrix.col, coo_matrix.data)\n",
    "    return sorted(tuples, key=lambda x: (x[1], x[0]), reverse=True)\n",
    "\n",
    "def extract_topn_from_vector(feature_names, sorted_items, topn=10):\n",
    "    \"\"\"get the feature names and tf-idf score of top n items\"\"\"\n",
    "    \n",
    "    #use only topn items from vector\n",
    "    sorted_items = sorted_items[:topn]\n",
    "\n",
    "    score_vals = []\n",
    "    feature_vals = []\n",
    "\n",
    "    for idx, score in sorted_items:\n",
    "        fname = feature_names[idx]\n",
    "        \n",
    "        #keep track of feature name and its corresponding score\n",
    "        score_vals.append(round(score, 3))\n",
    "        feature_vals.append(feature_names[idx])\n",
    "\n",
    "    #create a tuples of feature,score\n",
    "    #results = zip(feature_vals,score_vals)\n",
    "    results= {}\n",
    "    for idx in range(len(feature_vals)):\n",
    "        results[feature_vals[idx]]=score_vals[idx]\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "firefox keeps going back to previous page randomly , but i have not changed anything\n",
      "\n",
      "=====Body=====\n",
      "<p>i have the same OS and all else is same, but recently firefox keeps going back to the previous page randomly , and i have to click that forward arrow to get it back. a real pain as it reloads and i have to start over from what i was reading/copy-pasting etc.\n",
      "Perhaps it's a bug with a recent firefox update?\n",
      "</p>\n",
      "\n",
      "===Keywords===\n",
      "randomly 0.367\n",
      "back 0.287\n",
      "keeps 0.269\n",
      "previous 0.268\n",
      "going 0.259\n",
      "reloads 0.237\n",
      "pasting 0.233\n",
      "pain 0.209\n",
      "perhaps 0.195\n",
      "reading 0.185\n"
     ]
    }
   ],
   "source": [
    "# you only needs to do this once\n",
    "feature_names=cv.get_feature_names()\n",
    "\n",
    "# get the document that we want to extract keywords from\n",
    "doc=docs_test[0]\n",
    "\n",
    "#generate tf-idf for the given document\n",
    "tf_idf_vector=tfidf_transformer.transform(cv.transform([doc]))\n",
    "\n",
    "#sort the tf-idf vectors by descending order of scores\n",
    "sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "#extract only the top n; n here is 10\n",
    "keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "\n",
    "# now print the results\n",
    "print(\"\\n=====Title=====\")\n",
    "print(docs_title[0])\n",
    "print(\"\\n=====Body=====\")\n",
    "print(docs_body[0])\n",
    "print(\"\\n===Keywords===\")\n",
    "for k in keywords:\n",
    "    print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the common code into several methods\n",
    "def get_keywords(idx):\n",
    "\n",
    "    #generate tf-idf for the given document\n",
    "    tf_idf_vector=tfidf_transformer.transform(cv.transform([docs_test[idx]]))\n",
    "\n",
    "    #sort the tf-idf vectors by descending order of scores\n",
    "    sorted_items=sort_coo(tf_idf_vector.tocoo())\n",
    "\n",
    "    #extract only the top n; n here is 10\n",
    "    keywords=extract_topn_from_vector(feature_names,sorted_items,10)\n",
    "    \n",
    "    return keywords\n",
    "\n",
    "def print_results(idx,keywords):\n",
    "    # now print the results\n",
    "    print(\"\\n=====Title=====\")\n",
    "    print(docs_title[idx])\n",
    "    print(\"\\n=====Body=====\")\n",
    "    print(docs_body[idx])\n",
    "    print(\"\\n===Keywords===\")\n",
    "    for k in keywords:\n",
    "        print(k,keywords[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=====Title=====\n",
      "Firefox crashes\n",
      "\n",
      "=====Body=====\n",
      "<p>I am having crashes in FF 15.0.1 when I use Outlook Web Access. It usually happens when I change folders or have an item open and go back to the main OWA screen. This is happening many times a day and I always submit a crash report. I have put the latest crash report ID below. Please help as I rely on OWA for my work email.\n",
      "</p>\n",
      "\n",
      "===Keywords===\n",
      "owa 0.539\n",
      "report 0.299\n",
      "crash 0.258\n",
      "rely 0.23\n",
      "crashes 0.229\n",
      "submit 0.186\n",
      "outlook 0.182\n",
      "item 0.171\n",
      "main 0.157\n",
      "folders 0.157\n"
     ]
    }
   ],
   "source": [
    "idx=220\n",
    "keywords=get_keywords(idx)\n",
    "print_results(idx,keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier/Annontator Training step\n",
    "\n",
    "Describe the model and the model training step\n",
    "\n",
    "- Include a description the feature space used\n",
    "- Include a description of the selected classification or annotation model\n",
    "- Describe the training process and expected runtime for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'code that executes model training step'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"code that executes model training step\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifier/Annotator Testing step\n",
    "\n",
    "Describe the testing of the trained model's performace against a defined test set.\n",
    "\n",
    "- Include the raw performance\n",
    "- Include the source of ground truth for the evaluation\n",
    "- Include figures for FP/FN/ROC type metrics describing the model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'code that executes the model testing step'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"code that executes the model testing step\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "Sumamrize the model performance and findings related to specific misclassified items also a breif description of the findings as they correpsonde to generalizability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
